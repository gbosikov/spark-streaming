{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f432686-2375-44a1-aa08-254bbfe6f2df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1052f9f5-2ea9-4769-b83c-d092efa8745d",
   "metadata": {},
   "source": [
    "Из докера: https://hub.docker.com/_/cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60785786-0f07-41bf-8c94-4bfab03677d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cqlsh> DROP KEYSPACE IF EXISTS lesson6;\n",
    "cqlsh> CREATE KEYSPACE IF NOT EXISTS lesson6 WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor': 1 };\n",
    "cqlsh> USE lesson6;\n",
    "cqlsh:lesson6> CREATE TABLE IF NOT EXISTS db (\n",
    "           ...     id int, \n",
    "           ...     name text,\n",
    "           ...     size text,\n",
    "           ...     primary key (id)\n",
    "           ... );\n",
    "cqlsh:lesson6> INSERT INTO db (id, name, size) VALUES (3, 'Deer', 'Big');\n",
    "cqlsh:lesson6> SELECT * FROM db;\n",
    "\n",
    " id | name | size\n",
    "----+------+------\n",
    "  3 | Deer |  Big\n",
    "\n",
    "(1 rows)\n",
    "cqlsh:lesson6> INSERT INTO db (id, name) VALUES (3, 'Doe');\n",
    "cqlsh:lesson6> SELECT * FROM db;\n",
    "\n",
    " id | name | size\n",
    "----+------+------\n",
    "  3 |  Doe |  Big\n",
    "\n",
    "(1 rows)\n",
    "cqlsh:lesson6> INSERT INTO db (id, name) VALUES (5, 'Snake');\n",
    "cqlsh:lesson6> SELECT * FROM db;\n",
    "\n",
    " id | name  | size\n",
    "----+-------+------\n",
    "  5 | Snake | null\n",
    "  3 |   Doe |  Big\n",
    "\n",
    "(2 rows)\n",
    "cqlsh:lesson6> DELETE id FROM db where id = 3; -- вызывает ошибку - нельзя удалять в консоли по ключу. Только по значению\n",
    "InvalidRequest: Error from server: code=2200 [Invalid query] message=\"Invalid identifier id for deletion (should not be a PRIMARY KEY part)\"\n",
    "cqlsh:lesson6> INSERT INTO db (id, name, size) VALUES (3, null, null);\n",
    "cqlsh:lesson6> SELECT * FROM db;\n",
    "\n",
    " id | name  | size\n",
    "----+-------+------\n",
    "  5 | Snake | null\n",
    "  3 |  null | null\n",
    "\n",
    "(2 rows)\n",
    "cqlsh:lesson6> SELECT count(*) FROM db;\n",
    "\n",
    " count\n",
    "-------\n",
    "     2\n",
    "\n",
    "(1 rows)\n",
    "\n",
    "Warnings :\n",
    "Aggregation query used without partition key\n",
    "\n",
    "cqlsh:lesson6> drop table db;\n",
    "cqlsh:lesson6> SELECT * FROM db; -- вызывает ошибку - нет такой таблицы\n",
    "InvalidRequest: Error from server: code=2200 [Invalid query] message=\"table db does not exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b60f4592-1cab-4bd7-93d0-ba64e868d7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- name: string (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      "\n",
      "+---+-----+----+\n",
      "| id| name|size|\n",
      "+---+-----+----+\n",
      "|  5|Snake|null|\n",
      "|  3| Deer| Big|\n",
      "|  4|  Doe|null|\n",
      "+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--master local --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,com.datastax.spark:spark-cassandra-connector_2.12:3.2.0 pyspark-shell'\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType\n",
    "from pyspark.sql.functions import from_json, to_json, col, struct\n",
    "import time\n",
    "import signal\n",
    "from typing import Union\n",
    "\n",
    "from tools_kafka import Kafka\n",
    "from tools_pyspark_hdfs import Spark_HDFS as HDFS\n",
    "from tools_pyspark import stop_all_streams, sink, read_stream_kafka, console_stream, console_clear, console_show\n",
    "\n",
    "spark = SparkSession.builder.appName(\"my_spark\").getOrCreate()\n",
    "\n",
    "cass_animals_df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"db\", keyspace=\"lesson6\") \\\n",
    "    .load()\n",
    "\n",
    "cass_animals_df.printSchema()\n",
    "cass_animals_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c016933-35ab-482f-8e41-e9be910022e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "| id|name|size|\n",
      "+---+----+----+\n",
      "| 11| Cow| Big|\n",
      "+---+----+----+\n",
      "\n",
      "+---+-----+----+\n",
      "| id| name|size|\n",
      "+---+-----+----+\n",
      "|  5|Snake|null|\n",
      "| 11|  Cow| Big|\n",
      "|  3| Deer| Big|\n",
      "|  4|  Doe|null|\n",
      "+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# добавим строку в таблицу\n",
    "# для этого сначала создадим датафрейм\n",
    "cow_df = spark.sql(\"\"\"select 11 as id, \"Cow\" as name, \"Big\" as size \"\"\")\n",
    "cow_df.show()\n",
    "\n",
    "# запишем фрейм в таблицу\n",
    "cow_df.write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"db\", keyspace=\"lesson6\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "\n",
    "cass_animals_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b29c6e12-53e7-4c21-9398-4ae27f1f9b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "| id|name|size|\n",
      "+---+----+----+\n",
      "| 11| Cow| Big|\n",
      "+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cass_big_df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"db\", keyspace=\"lesson6\") \\\n",
    "    .load()\n",
    "\n",
    "cass_big_df.filter(F.col(\"name\")==\"Cow\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d25f967-8031-4ab3-8b2f-e32de57f7c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [id#107, name#108, size#109]\n",
      "+- BatchScan[id#107, name#108, size#109] Cassandra Scan: lesson6.db\n",
      " - Cassandra Filters: [[\"id\" = ?, 11]]\n",
      " - Requested Columns: [id,name,size] RuntimeFilters: []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Проверим пушит ли спарк фильтры в касандру.\n",
    "def explain(self, extended=True):\n",
    "    if extended:\n",
    "        print(self._jdf.queryExecution().toString())\n",
    "    else:\n",
    "        print(self._jdf.queryExecution().simpleString())\n",
    "\n",
    "cass_big_df.filter(F.col(\"id\")==11).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d42e2a43-897c-4785-9122-8e1ddbdb0528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [id#107, name#108, size#109]\n",
      "+- *(1) Filter ((id#107 >= 6) AND (id#107 <= 12))\n",
      "   +- BatchScan[id#107, name#108, size#109] Cassandra Scan: lesson6.db\n",
      " - Cassandra Filters: []\n",
      " - Requested Columns: [id,name,size] RuntimeFilters: []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Сделаем представление и используем его в выражении\n",
    "cass_big_df.createOrReplaceTempView(\"cass_df\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT * \n",
    "FROM cass_df\n",
    "WHERE id BETWEEN 6 and 12\n",
    "\"\"\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a184d5a2-1729-4404-955b-0be62feb2e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [id#107, name#108, size#109]\n",
      "+- BatchScan[id#107, name#108, size#109] Cassandra Scan: lesson6.db\n",
      " - Cassandra Filters: [[\"id\" IN (?, ?), (4, 11)]]\n",
      " - Requested Columns: [id,name,size] RuntimeFilters: []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Как видно из результата, фильтрацию будет делать не кассандра, а спарк, что медленно.\n",
    "# Перепишем запрос, чтобы запрос фильтровался на стороне базы\n",
    "spark.sql(\"\"\"\n",
    "SELECT * \n",
    "FROM cass_df\n",
    "WHERE id IN (4, 11)\n",
    "\"\"\").explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
