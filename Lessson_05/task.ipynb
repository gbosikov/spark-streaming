{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df01285-efbb-4cbf-ac05-04c549233aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== creating folder /data in hdfs ==========\n",
      "mkdir: `/data': File exists\n",
      "========== putting test file from local folder to hdfs ==========\n",
      "========== listing files in folder /data ==========\n",
      "Found 1 items\n",
      "-rw-r--r--   1 root supergroup         45 2022-12-07 12:56 /data/db\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rmr /data\n",
    "! hdfs dfs -put db2.csv /data/db2/file\n",
    "! hdfs dfs -put db.csv /data/db/file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2082e51-f397-484e-b2f3-4b118ae472b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 10:45:04 WARN Utils: Your hostname, alex-pc resolves to a loopback address: 127.0.1.1; using 192.168.122.1 instead (on interface virbr0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Не удалось удалить файл: /user/root\n",
      "{\"column_1\":\"a\",\"column_2\":2}\n",
      "{\"column_1\":\"b\",\"column_2\":4}\n",
      "{\"column_1\":\"c\",\"column_2\":8}\n",
      "{\"column_1\":\"a\",\"column_2\":2}\n",
      "{\"column_1\":\"b\",\"column_2\":4}\n",
      "{\"column_1\":\"c\",\"column_2\":8}\n",
      "{\"column_1\":\"b\",\"column_2\":4}\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType\n",
    "from pyspark.sql.functions import from_json, to_json, col, struct\n",
    "import time\n",
    "import signal\n",
    "from typing import Union\n",
    "\n",
    "from tools_kafka import Kafka\n",
    "from tools_pyspark_hdfs import Spark_HDFS as HDFS\n",
    "from tools_pyspark import stop_all_streams, sink, read_stream_kafka, console_stream, console_clear, console_show\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"my_spark\").getOrCreate()\n",
    "\n",
    "kf = Kafka()\n",
    "hdfs = HDFS(spark)\n",
    "checkpoint = 'checkpoints/duplicates_console_chk'\n",
    "topic_name = 'lesson4' # название топика для кафки\n",
    "kafka_server = kf.SERVERS\n",
    "schema = StructType() \\\n",
    "    .add(\"column_1\", StringType()) \\\n",
    "    .add(\"column_2\", IntegerType())\n",
    "\n",
    "#  удалим старые чекпойнты\n",
    "hdfs.rm('/user/root')\n",
    "\n",
    "\n",
    "if topic_name not in kf.ls():\n",
    "    # создадим топик кафки\n",
    "    kf.add(topic_name, retention=86400*30)\n",
    "\n",
    "    # отправим данные из файла в топик кафки\n",
    "    stream = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", True) \\\n",
    "        .option(\"maxFilesPerTrigger\", 1) \\\n",
    "        .schema(schema) \\\n",
    "        .csv(\"/data/db\") \\\n",
    "        .selectExpr(\"CAST(null AS STRING) as key\", \n",
    "                    \"CAST(to_json(struct(*)) AS STRING) as value\") \\\n",
    "        .writeStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", kf.SERVERS) \\\n",
    "        .option(\"topic\", topic_name) \\\n",
    "        .option(\"checkpointLocation\", \"checkpoints/stream_read_write\") \\\n",
    "        .start()\n",
    "\n",
    "# посмотрим содержимое топика кафки\n",
    "kf.get(topic_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2200b4-7f91-42ef-bd9e-8f4a5e1db288",
   "metadata": {},
   "source": [
    "## Как изменяется размер чекпойнта со временем на примере работы функции консоли"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75c02ec5-100f-482e-ba9c-d4cc9180f393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- column_1: string (nullable = true)\n",
      " |-- column_2: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      "\n",
      "Тип\tразмер\t\tпуть\n",
      "dir\t45.0 B\t\thdfs://localhost/user/root/console\n",
      "Тип\tразмер\t\tпуть\n",
      "dir\t5.6 KiB\t\thdfs://localhost/user/root/console\n",
      "Тип\tразмер\t\tпуть\n",
      "dir\t6.6 KiB\t\thdfs://localhost/user/root/console\n",
      "Stopping stream: <pyspark.sql.streaming.StreamingQuery object at 0x7454b92d4af0>\n"
     ]
    }
   ],
   "source": [
    "# Сделаем поток из топика кафки\n",
    "stream = read_stream_kafka(spark, kafka_server, topic_name, schema)\n",
    "stream.printSchema()\n",
    "\n",
    "# запускаем поток с выводом в консоль\n",
    "stream = console_stream(stream)\n",
    "\n",
    "# проверям как изменяется чекпойнт\n",
    "for i in range(3):\n",
    "    # смотрим как растёт чекпойнт\n",
    "    hdfs.du('/user/root')\n",
    "    # ждём работы вотермарка\n",
    "    time.sleep(5)\n",
    "\n",
    "# остановим поток\n",
    "stop_all_streams(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8006c776-bd4e-465c-9f5f-3403a2db79b0",
   "metadata": {},
   "source": [
    "## Что в консоли:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b66e6930-11b3-4c05-8416-b3ed2e1b310f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+\n",
      "|column_1|column_2|offset|\n",
      "+--------+--------+------+\n",
      "|c       |8       |5     |\n",
      "|a       |2       |0     |\n",
      "|b       |4       |6     |\n",
      "|b       |4       |4     |\n",
      "|b       |4       |1     |\n",
      "|a       |2       |3     |\n",
      "|c       |8       |2     |\n",
      "+--------+--------+------+\n",
      "\n",
      "Файл успешно удалён: console\n",
      "Не удалось удалить файл: console/checkpoint\n"
     ]
    }
   ],
   "source": [
    "# проверяем содержимое консоли\n",
    "console_show(spark)\n",
    "# удаляем файловую консоль\n",
    "console_clear(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ebbc99-cc4b-49bc-b633-cde162e83f13",
   "metadata": {},
   "source": [
    "## Используем окно, чтобы удалить дубликаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52df2c90-3d4a-4bc0-9fff-d7717ed0fd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- column_1: string (nullable = true)\n",
      " |-- column_2: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- receive_time: timestamp (nullable = false)\n",
      " |-- window_time: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      "\n",
      "+--------+--------+------+------------------------+--------------------------------------------------------------------+\n",
      "|column_1|column_2|offset|receive_time            |window_time                                                         |\n",
      "+--------+--------+------+------------------------+--------------------------------------------------------------------+\n",
      "|a       |2       |0     |2022-12-07T16:01:30.970Z|{start -> 2022-12-07T16:00:00.000Z, end -> 2022-12-07T16:02:00.000Z}|\n",
      "+--------+--------+------+------------------------+--------------------------------------------------------------------+\n",
      "\n",
      "+--------+--------+------+------------------------+--------------------------------------------------------------------+\n",
      "|column_1|column_2|offset|receive_time            |window_time                                                         |\n",
      "+--------+--------+------+------------------------+--------------------------------------------------------------------+\n",
      "|a       |2       |0     |2022-12-07T16:01:30.970Z|{start -> 2022-12-07T16:00:00.000Z, end -> 2022-12-07T16:02:00.000Z}|\n",
      "+--------+--------+------+------------------------+--------------------------------------------------------------------+\n",
      "\n",
      "+--------+--------+------+------------------------+--------------------------------------------------------------------+\n",
      "|column_1|column_2|offset|receive_time            |window_time                                                         |\n",
      "+--------+--------+------+------------------------+--------------------------------------------------------------------+\n",
      "|c       |8       |2     |2022-12-07T16:02:12.743Z|{start -> 2022-12-07T16:02:00.000Z, end -> 2022-12-07T16:04:00.000Z}|\n",
      "|a       |2       |0     |2022-12-07T16:01:30.970Z|{start -> 2022-12-07T16:00:00.000Z, end -> 2022-12-07T16:02:00.000Z}|\n",
      "|b       |4       |1     |2022-12-07T16:01:53.691Z|{start -> 2022-12-07T16:00:00.000Z, end -> 2022-12-07T16:02:00.000Z}|\n",
      "+--------+--------+------+------------------------+--------------------------------------------------------------------+\n",
      "\n",
      "+--------+--------+------+------------------------+--------------------------------------------------------------------+\n",
      "|column_1|column_2|offset|receive_time            |window_time                                                         |\n",
      "+--------+--------+------+------------------------+--------------------------------------------------------------------+\n",
      "|c       |8       |2     |2022-12-07T16:02:12.743Z|{start -> 2022-12-07T16:02:00.000Z, end -> 2022-12-07T16:04:00.000Z}|\n",
      "|a       |2       |0     |2022-12-07T16:01:30.970Z|{start -> 2022-12-07T16:00:00.000Z, end -> 2022-12-07T16:02:00.000Z}|\n",
      "|b       |4       |1     |2022-12-07T16:01:53.691Z|{start -> 2022-12-07T16:00:00.000Z, end -> 2022-12-07T16:02:00.000Z}|\n",
      "+--------+--------+------+------------------------+--------------------------------------------------------------------+\n",
      "\n",
      "+--------+--------+------+------------------------+--------------------------------------------------------------------+\n",
      "|column_1|column_2|offset|receive_time            |window_time                                                         |\n",
      "+--------+--------+------+------------------------+--------------------------------------------------------------------+\n",
      "|c       |8       |2     |2022-12-07T16:02:12.743Z|{start -> 2022-12-07T16:02:00.000Z, end -> 2022-12-07T16:04:00.000Z}|\n",
      "|b       |4       |4     |2022-12-07T16:02:51.276Z|{start -> 2022-12-07T16:02:00.000Z, end -> 2022-12-07T16:04:00.000Z}|\n",
      "|a       |2       |0     |2022-12-07T16:01:30.970Z|{start -> 2022-12-07T16:00:00.000Z, end -> 2022-12-07T16:02:00.000Z}|\n",
      "|a       |2       |3     |2022-12-07T16:02:32.937Z|{start -> 2022-12-07T16:02:00.000Z, end -> 2022-12-07T16:04:00.000Z}|\n",
      "|b       |4       |1     |2022-12-07T16:01:53.691Z|{start -> 2022-12-07T16:00:00.000Z, end -> 2022-12-07T16:02:00.000Z}|\n",
      "+--------+--------+------+------------------------+--------------------------------------------------------------------+\n",
      "\n",
      "Stopping stream: <pyspark.sql.streaming.StreamingQuery object at 0x769d70273df0>\n",
      "Файл успешно удалён: console\n"
     ]
    }
   ],
   "source": [
    "# Сделаем поток из топика кафки\n",
    "stream = read_stream_kafka(spark, kafka_server, topic_name, schema)\n",
    "\n",
    "# Добавим колонку receive_time о времени получения данных, чтобы работал вотермарк.\n",
    "# Используем метод withWatermark: параметр - назване колонки, второй параметр - \n",
    "# гарантированное время жизни информации о сообщении в чекпойнте.\n",
    "# Одновременно с этим будем удалять дубликаты в колонках\n",
    "# в drop_duplicates надо указывать колонку и время марки, иначе в чекпойнт будут попадать все данные.\n",
    "stream = stream.withColumn(\"receive_time\", F.current_timestamp()) \\\n",
    "                .withColumn(\"window_time\", F.window(F.col(\"receive_time\"), \"2 minutes\")) \\\n",
    "                .withWatermark(\"window_time\", \"2 minutes\") \\\n",
    "                .drop_duplicates(['column_1', 'window_time'])\n",
    "\n",
    "stream.printSchema()\n",
    "\n",
    "# запускаем поток с выводом в консоль\n",
    "stream = console_stream(stream)\n",
    "\n",
    "for i in range(5):\n",
    "    # ждём\n",
    "    time.sleep(5)\n",
    "    # проверяем содержимое консоли\n",
    "    console_show(spark)\n",
    "    \n",
    "# останавливаем потоки\n",
    "stop_all_streams(spark)\n",
    "# удаляем файловую консоль\n",
    "console_clear(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4a4c5a-fefd-4fd5-bf8f-3a9597de5ca8",
   "metadata": {},
   "source": [
    "## Сделаем группировку по sliding_time - не понятно, почему оно не работет!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af8c20df-b712-4551-8a64-e36f8913c512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sliding_time: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n",
      "+-----------------------------------+\n",
      "|Нет данных для построения DataFrame|\n",
      "+-----------------------------------+\n",
      "|Было обработано файлов: 0          |\n",
      "+-----------------------------------+\n",
      "\n",
      "Файл успешно удалён: console\n",
      "Stopping stream: <pyspark.sql.streaming.StreamingQuery object at 0x7979b84aecb0>\n"
     ]
    }
   ],
   "source": [
    "# Читаем поток из топика кафки\n",
    "stream = read_stream_kafka(spark, kafka_server, topic_name, schema, maxOffsetsPerTrigger=10)\n",
    "\n",
    "# сгруппируем по sliding_time\n",
    "stream = stream.select(\"column_1\", 'column_2') \\\n",
    "                .withColumn(\"receive_time\", F.current_timestamp()) \\\n",
    "                .withColumn(\"sliding_time\", F.window(F.col(\"receive_time\"), \"1 minute\", \"30 seconds\")) \\\n",
    "                .withWatermark(\"sliding_time\", \"2 minutes\") \\\n",
    "                .groupBy(\"sliding_time\").count()\n",
    "\n",
    "stream.printSchema()\n",
    "\n",
    "# запускаем поток с выводом в консоль\n",
    "stream = console_stream(stream, processingTime='0 seconds', mode='append')\n",
    "\n",
    "# ждём\n",
    "time.sleep(20)\n",
    "# проверяем содержимое консоли\n",
    "console_show(spark)\n",
    "\n",
    "# удаляем файловую консоль\n",
    "console_clear(spark)\n",
    "# останавливаем потоки\n",
    "stop_all_streams(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84020643-828b-4e55-a459-3ae9d1ff2ea9",
   "metadata": {},
   "source": [
    "## Join датафрейма к стриму"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "373895ea-5a98-436b-85e1-ed6129e3667c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- column_1: string (nullable = true)\n",
      " |-- column_2: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- receive_time: timestamp (nullable = false)\n",
      " |-- window_time: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- joined_value: string (nullable = true)\n",
      "\n",
      "+--------+--------+------+------------------------+--------------------------------------------------------------------+------------+\n",
      "|column_1|column_2|offset|receive_time            |window_time                                                         |joined_value|\n",
      "+--------+--------+------+------------------------+--------------------------------------------------------------------+------------+\n",
      "|c       |8       |2     |2022-12-08T10:10:39.096Z|{start -> 2022-12-08T10:10:00.000Z, end -> 2022-12-08T10:12:00.000Z}|3           |\n",
      "|b       |4       |1     |2022-12-08T10:10:39.096Z|{start -> 2022-12-08T10:10:00.000Z, end -> 2022-12-08T10:12:00.000Z}|9           |\n",
      "|a       |2       |0     |2022-12-08T10:10:39.096Z|{start -> 2022-12-08T10:10:00.000Z, end -> 2022-12-08T10:12:00.000Z}|7           |\n",
      "+--------+--------+------+------------------------+--------------------------------------------------------------------+------------+\n",
      "\n",
      "Stopping stream: <pyspark.sql.streaming.StreamingQuery object at 0x7979b84af790>\n",
      "Файл успешно удалён: console\n"
     ]
    }
   ],
   "source": [
    "# Создаём статичный фрейм, который будем джойнить к стриму\n",
    "static_df_schema = StructType() \\\n",
    "    .add(\"column_1\", StringType()) \\\n",
    "    .add(\"joined_value\", StringType())\n",
    "    \n",
    "static_df_data = (\n",
    "    (\"a\", 7),\n",
    "    (\"b\", 9),\n",
    "    (\"c\", 3),\n",
    ")\n",
    "\n",
    "static_df = spark.createDataFrame(static_df_data, static_df_schema)\n",
    "\n",
    "# Читаем поток из топика кафки\n",
    "stream = read_stream_kafka(spark, kafka_server, topic_name, schema, maxOffsetsPerTrigger=10)\n",
    "\n",
    "# делаем джойн внутри окна\n",
    "stream = stream.withColumn(\"receive_time\", F.current_timestamp()) \\\n",
    "                .withColumn(\"window_time\", F.window(F.col(\"receive_time\"), \"2 minutes\")) \\\n",
    "                .withWatermark(\"window_time\", \"2 minutes\") \\\n",
    "                .drop_duplicates(['column_1', 'window_time']) \\\n",
    "                .join(static_df, \"column_1\", \"left\")\n",
    "\n",
    "stream.printSchema()\n",
    "\n",
    "# запускаем поток с выводом в консоль\n",
    "stream = console_stream(stream, processingTime='0 seconds', mode='append')\n",
    "\n",
    "# ждём\n",
    "time.sleep(20)\n",
    "# проверяем содержимое консоли\n",
    "console_show(spark)\n",
    "    \n",
    "# останавливаем потоки\n",
    "stop_all_streams(spark)\n",
    "# удаляем файловую консоль\n",
    "console_clear(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed85e609-d536-472e-bd84-912a656c7155",
   "metadata": {},
   "source": [
    "## Join стрима к стриму"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d496a43-2839-4513-9b7a-396391653229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Схема данных в стриме из файла:\n",
      "root\n",
      " |-- column_1: string (nullable = true)\n",
      " |-- joined_value: integer (nullable = true)\n",
      " |-- receive_time_file: timestamp (nullable = false)\n",
      " |-- window_time: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      "\n",
      "Схема данных в стриме из топика kafka:\n",
      "root\n",
      " |-- column_1: string (nullable = true)\n",
      " |-- column_2: integer (nullable = true)\n",
      " |-- joined_value: integer (nullable = true)\n",
      " |-- receive_time: timestamp (nullable = false)\n",
      " |-- window_time: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      "\n",
      "+--------+--------+------------+------------------------+--------------------------------------------------------------------+\n",
      "|column_1|column_2|joined_value|receive_time            |window_time                                                         |\n",
      "+--------+--------+------------+------------------------+--------------------------------------------------------------------+\n",
      "|a       |2       |12          |2022-12-08T10:53:11.761Z|{start -> 2022-12-08T10:52:00.000Z, end -> 2022-12-08T10:54:00.000Z}|\n",
      "|a       |2       |32          |2022-12-08T10:53:11.761Z|{start -> 2022-12-08T10:52:00.000Z, end -> 2022-12-08T10:54:00.000Z}|\n",
      "|c       |8       |18          |2022-12-08T10:53:11.761Z|{start -> 2022-12-08T10:52:00.000Z, end -> 2022-12-08T10:54:00.000Z}|\n",
      "|c       |8       |82          |2022-12-08T10:53:11.761Z|{start -> 2022-12-08T10:52:00.000Z, end -> 2022-12-08T10:54:00.000Z}|\n",
      "|b       |4       |15          |2022-12-08T10:53:11.761Z|{start -> 2022-12-08T10:52:00.000Z, end -> 2022-12-08T10:54:00.000Z}|\n",
      "|b       |4       |21          |2022-12-08T10:53:11.761Z|{start -> 2022-12-08T10:52:00.000Z, end -> 2022-12-08T10:54:00.000Z}|\n",
      "|b       |4       |14          |2022-12-08T10:53:11.761Z|{start -> 2022-12-08T10:52:00.000Z, end -> 2022-12-08T10:54:00.000Z}|\n",
      "+--------+--------+------------+------------------------+--------------------------------------------------------------------+\n",
      "\n",
      "Stopping stream: <pyspark.sql.streaming.StreamingQuery object at 0x7bc7e0453880>\n",
      "Файл успешно удалён: console\n"
     ]
    }
   ],
   "source": [
    "# сделаем два одинаковых по данным стрима: один из csv файла, \n",
    "# воторой из топика кафки (который был наполнен из того же csv Файла)\n",
    "\n",
    "schema_join_file = StructType() \\\n",
    "    .add(\"column_1\", StringType()) \\\n",
    "    .add(\"joined_value\", IntegerType())\n",
    "\n",
    "stream_from_file = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .schema(schema_join_file) \\\n",
    "    .csv(\"/data/db2\") \\\n",
    "    .withColumn(\"receive_time_file\", F.current_timestamp()) \\\n",
    "    .withColumn(\"window_time\",F.window(F.col(\"receive_time_file\"), \"2 minutes\")) \\\n",
    "    .withWatermark(\"window_time\", \"2 minutes\")\n",
    "\n",
    "stream_from_kafka = read_stream_kafka(spark, kafka_server, topic_name, schema, maxOffsetsPerTrigger=10)\n",
    "\n",
    "# делаем джойн внутри окна\n",
    "stream_from_kafka = stream_from_kafka.withColumn(\"receive_time\", F.current_timestamp()) \\\n",
    "                .withColumn(\"window_time\", F.window(F.col(\"receive_time\"), \"2 minutes\")) \\\n",
    "                .withWatermark(\"window_time\", \"2 minutes\") \\\n",
    "                .drop_duplicates(['column_1', 'window_time']) \\\n",
    "                .join(stream_from_file, [\"column_1\", \"window_time\"] , \"inner\") \\\n",
    "                .select(\"column_1\", \"column_2\",  \"joined_value\", \"receive_time\", \"window_time\")\n",
    "\n",
    "print('Схема данных в стриме из файла:')\n",
    "stream_from_file.printSchema()\n",
    "print('Схема данных в стриме из топика kafka:')\n",
    "stream_from_kafka.printSchema()\n",
    "\n",
    "# запускаем поток с выводом в консоль\n",
    "stream_from_kafka = console_stream(stream_from_kafka, processingTime='0 seconds', mode='append')\n",
    "\n",
    "# ждём\n",
    "time.sleep(20)\n",
    "# проверяем содержимое консоли\n",
    "console_show(spark)\n",
    "\n",
    "# останавливаем потоки\n",
    "stop_all_streams(spark)\n",
    "# удаляем файловую консоль\n",
    "console_clear(spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
